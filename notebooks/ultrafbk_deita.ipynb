{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use DEITA Pipeline on UltraFBK dataset\n",
    "\n",
    "\n",
    "1. reformat `ultrafbk` to be in the `sharegpt` format (quite minimal work)\n",
    "2. pipe into DEITA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ultrafbk_dataset = load_dataset(\n",
    "    \"when2rl/UltraFeedback_binarized_cleaned_annotated\",\n",
    "    split='train_prefs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def reformat_to_sharegpt(data_dict):\n",
    "    old_conversations = data_dict['messages']\n",
    "    conversations = []\n",
    "    for i, turn in enumerate(old_conversations):\n",
    "        role = turn['role']\n",
    "        content = turn['content']\n",
    "        # sanity checks\n",
    "        if i % 2 == 0 and role != \"user\":\n",
    "            raise ValueError(f\"Expected user role, got {role}\")\n",
    "        if i % 2 == 1 and role != \"assistant\":\n",
    "            raise ValueError(f\"Expected assistant role, got {role}\")\n",
    "        # append\n",
    "        conversations.append({\n",
    "            'from': \"human\" if role == \"user\" else \"gpt\",\n",
    "            'value': content\n",
    "        })\n",
    "    data_dict['conversations'] = conversations\n",
    "\n",
    "    ### add full id\n",
    "    text_chosen = data_dict['chosen']\n",
    "    text_rejected = data_dict['rejected']\n",
    "    full_encoded = f\"{text_chosen} {text_rejected}\"\n",
    "    full_encoded_id = hashlib.sha256(full_encoded.encode(\"utf-8\")).hexdigest()\n",
    "    data_dict['full_id'] = full_encoded_id\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6231b17f60c645a2b98f0691d99d27eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reformatting to ShareGPT format (num_proc=8):   0%|          | 0/61135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ultrafbk_dataset_reformatted = ultrafbk_dataset.map(\n",
    "    reformat_to_sharegpt,\n",
    "    num_proc=8,\n",
    "    keep_in_memory=True,\n",
    "    desc=\"Reformatting to ShareGPT format\"\n",
    ")\n",
    "ultrafbk_dataset_reformatted = ultrafbk_dataset_reformatted.select_columns(\n",
    "    ['conversations', 'full_id', 'prompt_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'full_id', 'prompt_id'],\n",
       "    num_rows: 61135\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultrafbk_dataset_reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ultrafbk_dataset_reformatted.select(range(500)).to_pandas()\n",
    "tmp_df.to_json(\"../data/deita/tmp.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deita.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/deita/tmp.json\"\n",
    "complexity_scored_data_path = \"../data/deita/tmp_complexity.json\"\n",
    "embedding_path = \"../data/deita/tmp_embeddings.pkl\"\n",
    "output_path = \"../data/deita/tmp_deita.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    \"score_pipeline\",\n",
    "    data_path = data_path,   # json file with sharegpt format\n",
    "    scorer = \"mistral\",   # [mistral, llama]\n",
    "    scorer_name_or_path = \"hkust-nlp/deita-complexity-scorer\",  # scorer name or path e.g. hkust-nlp/deita-complexity-scorer\n",
    "    is_vllm = False,  # launch with vllm [True, False]\n",
    "    score_type = [\"complexity\", \"quality\"], # [complexity, quality]\n",
    "    output_path = complexity_scored_data_path\n",
    ")  # output path (json format)\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22ba509911a45b59e6777c24a91f8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b44368bbe44bcb9782cca7f7a4e2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and reformatting instruction data (num_proc=32):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([16, 1038])\n",
      "input_ids torch.Size([16, 515])\n",
      "input_idsinput_ids torch.Size([16, 525])\n",
      " torch.Size([16, 1249])\n",
      "input_ids torch.Size([16, 1657])\n",
      "input_ids input_idstorch.Size([16, 1445])\n",
      " torch.Size([16, 705])\n",
      "input_ids input_idstorch.Size([16, 1857])\n",
      " torch.Size([16, 894])\n",
      "input_ids torch.Size([16, 695])\n",
      "input_ids torch.Size([16, 813])\n",
      "input_ids torch.Size([16, 587])\n",
      "input_ids torch.Size([16, 1066])\n",
      "input_ids torch.Size([16, 786])\n",
      "input_ids\n",
      " input_idstorch.Size([16, 1811])\n",
      " torch.Size([16, 923])input_ids torch.Size([16, 928])\n",
      "input_idsinput_ids torch.Size([16, 1427])\n",
      " torch.Size([16, 993])\n",
      "input_ids torch.Size([16, 1430])\n",
      "input_ids\n",
      " torch.Size([15, 1657])input_ids torch.Size([15, 1246])\n",
      "input_idsinput_ids torch.Size([15, 810]) \n",
      "torch.Size([15, 2048])\n",
      "input_ids torch.Size([15, 1681])\n",
      "input_ids torch.Size([15, 1249])\n",
      "input_ids input_idstorch.Size([15, 2048])\n",
      " torch.Size([15, 1099])\n",
      "input_ids torch.Size([15, 1195])\n",
      "input_ids torch.Size([15, 1832])\n",
      "input_ids torch.Size([15, 1222])\n",
      "input_ids torch.Size([15, 1361])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:03<00:00,  7.92it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "parser.add_argument(\"--data_path\", type=str, default=None)\n",
    "parser.add_argument(\"--output_path\", type=str, default=None)\n",
    "parser.add_argument(\"--max_length\", type=int, default=2048)\n",
    "parser.add_argument(\"--batch_size_per_device\", type=int, default=4)\n",
    "parser.add_argument(\"--conv_template\", type=str, default=\"vicuna_v1.1\")\n",
    "parser.add_argument(\"--use_flash_attention\", type=bool, default=False)\n",
    "parser.add_argument(\"--only_answer\", type=bool, default=False)\n",
    "parser.add_argument(\"--random_shuffle\", type=bool, default=False)\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=\"mistralai/Mistral-7B-v0.1\")\n",
    "\"\"\"\n",
    "\n",
    "embed_pipeline = Pipeline(\n",
    "    \"embed_pipeline\", \n",
    "    data_path = data_path,   # json file with sharegpt format\n",
    "    output_path = embedding_path,  # output path (pickle format)\n",
    "    model_name_or_path = \"mistralai/Mistral-7B-v0.1\",  # model name or path e.g. mistralai/Mistral-7B-v0.1\n",
    "    max_length = 2048,\n",
    "    use_flash_attention = True,\n",
    "    batch_size_per_device = 1,\n",
    "    conv_template = \"vicuna_v1.1\",\n",
    "    only_answer = False,\n",
    "    random_shuffle = False,\n",
    "    bfloat16 = True\n",
    ")\n",
    "\n",
    "embed_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'complexity_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'complexity_scores'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m filter_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m data_path,  \u001b[38;5;66;03m# json file with sharegpt format\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# GPU IDX, default: 0\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[43mfilter_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/when2rl/deita/notebooks/../src/deita/pipeline/base.py:51\u001b[0m, in \u001b[0;36mBasePipeline.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother_data_path\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     49\u001b[0m     other_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_other_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mother_data_path)\n\u001b[0;32m---> 51\u001b[0m preprocessed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(preprocessed_data)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_data(json_data, results)\n",
      "File \u001b[0;32m/workspace/when2rl/deita/notebooks/../src/deita/pipeline/filter_pipeline.py:66\u001b[0m, in \u001b[0;36mFilterPipeline._preprocess\u001b[0;34m(self, json_data, other_data)\u001b[0m\n\u001b[1;32m     63\u001b[0m df_json \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(json_data)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key:\n\u001b[0;32m---> 66\u001b[0m     df_data[sk] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_json\u001b[49m\u001b[43m[\u001b[49m\u001b[43msk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'complexity_scores'"
     ]
    }
   ],
   "source": [
    "filter_pipeline = Pipeline(\n",
    "    \"filter_pipeline\", \n",
    "    data_path = data_path,  # json file with sharegpt format\n",
    "    other_data_path = embedding_path,  # embedding file path (pickle format)\n",
    "    threshold = 0.9,  # filter threshold default: 0.9 \n",
    "    data_size = 100,  # size of selected data\n",
    "    chunk_size = 100000,  # used for more efficient GPU computing  default: 100000\n",
    "    sort_key = \"complexity_scores,quality_scores\",  # default: \"complexity_scores,quality_scores\"\n",
    "    output_path = output_path,  # json format output path\n",
    "    distance_metric = \"cosine\",  # default: cosine\n",
    "    embedding_field = \"embedding\",  # default: embedding\n",
    "    is_compression = False,  # default: False\n",
    "    device = 0  # GPU IDX, default: 0\n",
    ")\n",
    "\n",
    "filter_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
